{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**Classical Statistics: Maximum Likelihood Estimation**</font>\n",
    "\n",
    "\n",
    "# 1. RR Lyrae stars in M4\n",
    "\n",
    "RR Lyrae stars are variable pulsators that obey a precise period-luminosity relation. Here we have a sample of RR Lyrae stars in the globular cluster M4, observed with Spitzer by Neeley et al. (2015): http://cdsads.u-strasbg.fr/abs/2015ApJ...808...11N\n",
    "\n",
    "Since the stars are at the same distance, then the apparent magnitude will also depend on the period::\n",
    "\n",
    "$$ m = a\\ {\\rm log}\\ P + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_lyrae_table2 = Table.read('./data/table2.dat', readme='./data/ReadMe', format='cds')\n",
    "RR_lyrae = RR_lyrae_table2[RR_lyrae_table2['Mode'] == 'RRab']\n",
    "\n",
    "# Now remove sources V20 and V21 due to blending\n",
    "RR_lyrae = RR_lyrae[RR_lyrae['ID'] != 'V20']\n",
    "RR_lyrae = RR_lyrae[RR_lyrae['ID'] != 'V21']\n",
    "\n",
    "print(\"Columns:\", RR_lyrae.colnames)\n",
    "logP, m36, e_m36 = RR_lyrae[\"logP\"], RR_lyrae['[3.6]'], RR_lyrae['e_[3.6]']\n",
    "logP, m36, e_m36 = logP[~m36.mask], m36[~m36.mask], e_m36[~m36.mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. The data\n",
    "\n",
    "Let's plot the data from the 3.6Î¼ band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(np.min(logP), np.max(logP), 40)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(logP, m36, yerr=e_m36, fmt=\"b.\", capsize=4, label=\"Data\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\log P$\")\n",
    "plt.ylabel(r\"$m_{3.6}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Quick Python code for fitting\n",
    "\n",
    "1. `numpy.polyfit` method for fitting polynomials\n",
    "\n",
    "> It actually performs an ordinary least squares fit, obviously without taking into account the uncertainties \n",
    "\n",
    "2. `scipy.optimize.minimize` can minimize any function\n",
    "\n",
    "> ...so we can apply it to minimize the $\\chi^2$ (using the uncertainties). Note that the optimization method is selected because it's not always successful\n",
    "\n",
    "3. **A trick**\n",
    "\n",
    "> $\\chi^2$ fitting can be seen as a *weighted version* of ordinary least squares. The weight are applied on the unsquared residuals: $ w_i \\left[y_i - \\hat{y}(x_i)\\right]$.\n",
    "By comparing with the $\\chi^2$ formula we can use $1/\\sigma_i$ as weights in `numpy.polyfit` where $\\sigma_i$ are the uncertainties of each data point.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "**Tasks:** \n",
    "    \n",
    "1. Make `np.polyfit` work\n",
    "2. Select the initial guess for minimizing `chi_square`\n",
    "3. Make `np.polyfit` work with the correct weights\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary least squares fit\n",
    "\n",
    "ols_params = np.polyfit(logP, m36, deg=...)\n",
    "print(\"Polyfit output:\")\n",
    "print(ols_params)\n",
    "print()\n",
    "\n",
    "\n",
    "# chi-square fitting using minimization\n",
    "\n",
    "def chi_square(params):\n",
    "    a, b = params\n",
    "    m36_model = a * logP + b\n",
    "    return np.sum((m36 - m36_model) ** 2.0 / e_m36**2.0)\n",
    "\n",
    "min_result = minimize(chi_square, x0=..., method=\"Nelder-Mead\")\n",
    "print(\"Minimization output:\")\n",
    "print(min_result)\n",
    "print(\"Optimal Parameters:\")\n",
    "print(min_result.x)\n",
    "\n",
    "\n",
    "# chi-square fitting using weighted ordinary least squares fit: w = 1/sigma here!\n",
    "\n",
    "ch2_params = np.polyfit(logP, m36, deg=..., w=...)\n",
    "print()\n",
    "print(\"chi-square fitting using weighted polyfit:\")\n",
    "print(ch2_params)\n",
    "\n",
    "\n",
    "# plotting the results\n",
    "\n",
    "xx = np.linspace(np.min(logP), np.max(logP), 40)\n",
    "plt.figure()\n",
    "plt.errorbar(logP, m36, yerr=e_m36, fmt=\"b.\", capsize=4, label=\"Data\")\n",
    "plt.plot(xx, np.polyval(ols_params, xx), \"k-\", lw=2, label=f\"OLS:\\t $m_{{3.6}} = ${ols_params[0]:.3g} $\\log P$ + {ols_params[1]:.3g}]\")\n",
    "plt.plot(xx, np.polyval(ch2_params, xx), \"r-\", label=f\"$\\chi^2$:\\t $m_{{3.6}} = ${ch2_params[0]:.3g} $\\log P$ + {ch2_params[1]:.3g}]\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\log P$\")\n",
    "plt.ylabel(r\"$m_{3.6}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Constructing a likelihood function\n",
    "\n",
    "Our problem falls into the general case where we have a model connecting two quantities through a function with two parameters, $a$ and $b$: $y=f(x;a,b)=ax + b$.\n",
    "\n",
    "In the general case, we use $\\theta$ as a list of parameters: $\\theta = \\{\\theta_1, \\theta_2, \\cdots, \\theta_k\\}$: \n",
    "\n",
    "$$ \\Large y = f(x; \\theta) $$\n",
    "\n",
    "Therefore, if we get $N$ data, $(x_i, y_i)$ for $i \\in [1, 2, \\cdots, N]$ based on the model we would expect:\n",
    "\n",
    "$$\\Large y_i = f(x_i; \\theta) $$\n",
    "\n",
    "However, all observations are **subject to uncertainty**, and we need to model this as well. More often than not, uncertainties are fluctuations of a certain magnitude $\\sigma$ around 0, following the Gaussian distribution:\n",
    "\n",
    "$$\\Large y_i = f(x_i; \\theta) + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i$ is normally distributed:\n",
    "\n",
    "$$\\Large \\epsilon_i \\sim \\mathrm{Norm}(0, \\sigma_i)$$\n",
    "\n",
    "The $\\sigma_i$ is the standard deviation of the datum error distribution, or an **estimate** of the typical difference between the observed $y_i$ and the intrinsic, true value $y_i$ which we **assume that is described by the model**.\n",
    "\n",
    "Consequently, according to our model, the probability to observe the $i$-th point's $y$-value given it's $x$-value, or the **datum likelihood** is\n",
    "\n",
    "$$\\Large\n",
    "P(y_i | x_i) = \\text{Norm}(y_i; f(\\theta; x_i), \\sigma_i) \n",
    "   = \\dfrac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp \\left[{-\\dfrac{(y_i-f(x_i; \\theta))^2}{2\\sigma_i^2}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our measurements are independent (the probability of $y_2$ does not depend on $y_1$ or $x_3$), the overall probability to get our data, or **likelihood** (always **according to our model**) is the product of all likelihoods:\n",
    "\n",
    "$$\\Large L = P(y_1, y_2, \\cdots, y_N | x_1, x_2, \\cdots x_N) = \\prod_{i=1}^{N} P(y_i | x_i)$$\n",
    "\n",
    "Let's do the math...\n",
    "\n",
    "$$\\Large\n",
    "L = \\prod_{i=1}^{N} \\dfrac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp \\left[{-\\dfrac{(y_i-f(x_i))^2}{2\\sigma_i^2}} \\right]\n",
    "$$\n",
    "\n",
    "$$\\Large\n",
    "L = \\left(\\dfrac{1}{\\sqrt{2\\pi \\sigma_i^2}}\\right)^N \\exp \\left[ -\\sum_{i=1}^{N} {\\dfrac{(y_i-f(x_i))^2}{2\\sigma_i^2}} \\right]\n",
    "$$\n",
    "\n",
    "The parameters that maximize the likelihood are found by taking\n",
    "\n",
    "$$\\Large \\frac{\\partial L}{\\partial a} = 0 \\qquad\\text{and}\\qquad \\Large \\frac{\\partial L}{\\partial b} = 0$$\n",
    "\n",
    "\n",
    "## The log trick\n",
    "\n",
    "The likelihoods are typicall very small quantities. To avoid numerical issues, we are allowd to work in log-space (they are positive):\n",
    "\n",
    "$$\\Large\n",
    "l = \\ln{L} = -\\frac{N}{2} \\ln\\left(2\\pi \\sigma^2\\right) - \\sum_{i=1}^{N} {\\dfrac{(y_i-f(x_i))^2}{2\\sigma_i^2}}\n",
    "$$\n",
    "\n",
    "Not only it simplifies the equation, but it's convenient for maximization with respect to any parameter $\\theta_j$:\n",
    "\n",
    "$$ \\Large                      \\frac{\\partial \\ln L}{\\partial \\theta_j}        = 0 \n",
    "   \\quad\\Leftrightarrow\\quad   \\frac{1}{L}\\frac{\\partial L}{\\partial \\theta_j} = 0 \n",
    "   \\quad\\Leftrightarrow\\quad   \\frac{\\partial L}{\\partial \\theta_j}            = 0 $$\n",
    "\n",
    "Therefore, for our example we can maximizing the log-likelihood with respect to the two parameters:\n",
    "\n",
    "$$\\Large \\frac{\\partial l}{\\partial a} = 0 \\qquad\\text{and}\\qquad \\Large \\frac{\\partial l}{\\partial b} = 0.$$\n",
    "\n",
    "\n",
    "## Connection with $\\chi^2$\n",
    "\n",
    "The $l$ can be written as\n",
    "\n",
    "$$\\Large\n",
    "l = \\text{constant} - \\frac{1}{2} \\sum_{i=1}^{N} {\\dfrac{(y_i-f(x_i))^2}{\\sigma_i^2}} = \\text{constant} - \\frac{\\chi^2}{2}\n",
    "$$\n",
    "\n",
    "\n",
    "Consequently, maximizing the likelihood is equivalent to minimizing the $\\chi^2$!\n",
    "\n",
    "\n",
    "\n",
    "## If the uncertainties are equal...\n",
    "\n",
    "If $\\sigma_i \\equiv \\sigma$, then... \n",
    "\n",
    "$$\\Large\n",
    "l = \\text{constant} - \\frac{1}{2} \\sum_{i=1}^{N} {\\dfrac{(y_i-f(x_i))^2}{\\sigma_i^2}} =\n",
    "\\text{constant} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} {(y_i-f(x_i))^2}\n",
    "$$\n",
    "\n",
    "\n",
    "So maximizing the likelihood corresponds to minimizing the quantity\n",
    "\n",
    "$$\\Large \\sum_{i=1}^{N} (y_i-f(x_i))^2 $$\n",
    "\n",
    "...or the **sum of the squares**!\n",
    "\n",
    "In fact, using $f(x) = a x + b$ you will arrive at the known OLS formulae for linear fitting without using linear algebra at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Let's plot the likelihood close to the solution we found before\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "**Task:** Complete the `log_likelihood` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# construct a 2D grid with 101 points in each direction\n",
    "a_grid = np.sort(np.linspace(0.95 * ch2_params[0], 1.05 * ch2_params[0], 101))\n",
    "b_grid = np.sort(np.linspace(0.997 * ch2_params[1], 1.003 * ch2_params[1], 101))\n",
    "[A, B] = np.meshgrid(a_grid, b_grid)\n",
    "\n",
    "\n",
    "# construct the likelihood function (and vectorize it to be applied directly on arrays)\n",
    "@np.vectorize\n",
    "def log_likelihood(a, b):\n",
    "    return ...\n",
    "\n",
    "\n",
    "lnL = log_likelihood(A, B)    # compute the likelihood in each grid point\n",
    "lnL = lnL - np.max(lnL)       # normalizing it for the maximum to be 0.0\n",
    "L = np.exp(lnL)               # un-log it to better visualization\n",
    "\n",
    "# plot the results\n",
    "plt.figure()\n",
    "plt.contourf(A, B, L, 100, cmap='gnuplot2')\n",
    "plt.plot(*ch2_params, \"xk\")\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "cbar = plt.colorbar(label=r\"$L / L_{\\max}$\")\n",
    "cbar.set_ticks(np.linspace(0, 1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exercise: fitting for slope only\n",
    "\n",
    "Let's assume that in the above relation, the intercept $b$ is known (it mainly depends on the distance of the objects, and a calibration of the relation):\n",
    "\n",
    "$$ m = a\\ {\\rm log}\\ P + 10.3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**In-class discussion: Can we fit the slope using standard methods for fitting a line (OLS)?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "We do not have an intercept any more. Even if we fit $m-10.3$, we might end up with a small \"remainder\". \n",
    "Even if we take the logs again, we would have $\\log(m-10.3) = \\log a + \\log\\log P$ which requires fitting for the intercept only.\n",
    "Perhaps, $a = (m - 10.3) / \\log P$ is telling us that from each point we have an estimate of the slope and we could average them.\n",
    "But none of these approaches takes into account the uncertainties.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_params_1param = np.polyfit(logP, m36-10.3, deg=1)\n",
    "print(\"Ordinary Least Squares result:     a={:.3f}, b={:.3g}\".format(*ols_params_1param))\n",
    "\n",
    "slope_estimates = (m36-10.3) / logP\n",
    "mean_slope = np.mean(slope_estimates)\n",
    "print(f\"Average of slope estimates:        a={mean_slope:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(np.min(logP), np.max(logP), 40)\n",
    "plt.figure()\n",
    "plt.errorbar(logP, m36, yerr=e_m36, fmt=\"b.\", capsize=4, label=\"Data\")\n",
    "plt.plot(xx, 10.3 + np.polyval(ols_params_1param, xx), \"k-\", lw=2, label=f\"OLS: $m_{{3.6}}$-10.3={ols_params_1param[0]:.3g} $\\log P$ {ols_params_1param[1]:+.3g}\")\n",
    "plt.plot(xx, 10.3 + mean_slope * xx, \"r-\", label=f\"$\\\\bar{{a}}$: $m_{{3.6}} - 10.3 = ${mean_slope:.3g} $\\log P$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\log P$\")\n",
    "plt.ylabel(r\"$m_{3.6}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**In-class discussion: Can we fit the slope using $\\chi^2$-fitting?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "    Sure! This includes the uncertainties, and we are using directly the likelihood we want.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_min_result = minimize(lambda a: np.sum((m36-10.3-a*logP)**2.0 / e_m36**2.0), x0=mean_slope)\n",
    "print(f\"Chi-square minimization estimate:  a={chi2_min_result.x[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**In-class discussion: What is the uncertainty on the slope?**</u><font>\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "Most methods (OLS, $\\chi^2$) have ways to get the uncertainties of the parameters. These are based on Gaussian approximations of the likelihood itself! \n",
    "Here we have the likelihood, why not use it?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Plotting the (log-)likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_1param(slope):\n",
    "    return -0.5*np.sum((m36-10.3 - slope*logP)**2.0 / e_m36**2.0)\n",
    "\n",
    "# the slope values to try\n",
    "a_values = a_grid.copy()\n",
    "\n",
    "# the corresponding log-likelihood values\n",
    "lnL_values = np.array([log_likelihood_1param(a_value) for a_value in a_values])\n",
    "\n",
    "# the likelihood values, normalized to 1\n",
    "L_values = np.exp(lnL_values - np.max(lnL_values))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(10, 3))\n",
    "ax1.plot(a_values, lnL_values, \"k-\")\n",
    "ax1.set_ylabel(r\"$\\ln L$\")\n",
    "ax2.plot(a_values, L_values, \"k-\")\n",
    "ax2.set_ylabel(r\"$L$\")\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlabel(\"Slope, $a$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Estimating the uncertainty through the CDF\n",
    "\n",
    "What does the *uncertainty on the slope* mean? The likelihood function gives all the information. Sigma values, upper and lower limits, are simply summarizing the likelihood using 1-2 values.\n",
    "\n",
    "We can **define** as 68% confidence interval on the slope, the region around the median, where the area under the (properly normalized) likelihood is 68%:\n",
    "\n",
    "$$\\Large \\int\\limits_{lo}^{up} L(a) da = 0.68 $$\n",
    "\n",
    "where $lo$ and $hi$ are the lower and upper bounds of the CI. Since we defined the CI as something around the median, this implies that the $lo$ and $up$ values correspond to the 16 and 84 percentiles. Therefore:\n",
    "\n",
    "$$\\Large \\int\\limits_{-\\infty}^{lo} L(a) da = 0.16 $$\n",
    "\n",
    "$$\\Large \\int\\limits_{-\\infty}^{up} L(a) da = 0.84 $$\n",
    "\n",
    "This is why the **cumulative density function** (CDF) is very useful! We can obtain the CDF by integrating the likelihood:\n",
    "\n",
    "$$\\Large F(a') = \\int\\limits_{-\\infty}^{a'} L(a) da $$\n",
    "\n",
    "Since we have calculated $f(x)$ on a regular and fine grid of $x$, i.e., $\\Delta x$ is constant and a small value. Consequently, the integral calculated in the same grid, is simply the **cumulative sum** of the likelihood values in the grid. Of course, we need to **normalize** it so that the CDF at infinity is 1. Since we started with a wide range of $x$ (including most of the likelihood), we can assume that the CDF goes from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = np.cumsum(L_values)\n",
    "cdf /= cdf[-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(a_values, cdf, \"k.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Finding the confidence interval\n",
    "\n",
    "The CDF calculated in the grid is a series of point. To find the value at which the CDF is say, 0.16, we can use interpolation! But not the typical one where we interpolate $y$ values from $x$ values, but the opposite!\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "**Task:** Use interpolation to find the confidence interval and the median of the slope.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo68, median, hi68 = np.interp(..., ..., ...)\n",
    "print(f\"Slope = {median:.3f} +{hi68-median:.3f} -{median-lo68:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.axhspan(0.16, 0.84, facecolor=\"0.75\", alpha=0.5, edgecolor=\"none\")\n",
    "plt.axvspan(lo68, hi68, facecolor=\"r\", alpha=0.5, edgecolor=\"none\")\n",
    "plt.axvline(median, color=\"r\")\n",
    "plt.axhline(0.50, color=\"0.5\", ls=\"-\")\n",
    "plt.plot(a_values, cdf, \"k-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional considerations\n",
    "\n",
    "1. The grid is not necessary. We can directly calculate any percentile by defining the CDF as a function that integrates (e.g., with `scipy.integrate`), and then using root finders or minimizers (e.g., with `scipy.optimize`), find the value for which it gives 0.16 or 0.84!\n",
    "\n",
    "2. We can directly use the likelihood to calculate other statistics using formulas applied on distribution functions:\n",
    "\n",
    "Mean: $ \\mu = \\int\\limits_{-\\infty}^{+\\infty} x L(x)\\, dx$\n",
    "\n",
    "Variance: $\\text{Var} = \\int\\limits_{-\\infty}^{+\\infty} \\left(x - \\mu\\right)^2 L(x) dx$\n",
    "\n",
    "Standard deviation: $\\sigma = \\sqrt{\\text{Var}}$\n",
    "\n",
    "...and so on! It involves integrations that are typically easy to calculate with standard methods (if the likelihood is smooth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hypothesis testing in the framework of correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dependence/independence of random variables\n",
    "Sometimes we want to test whether two quantities are (un)correlated, or *(in)dependent*. This is useful for \n",
    "* confirming a model predicting that such a correlation exists\n",
    "* predicting a quantity (e.g., 'y' from 'x')\n",
    "* verify a monotonic relation between two quantities\n",
    "\n",
    "## 3.2. Linear correlation tests\n",
    "\n",
    "We can perform an ordinary least-squares fit to get the best-fitting values of $a$ and $b$. The function `linregress` provided by the `scipy` package can also be used, as it also returns a $p$-value based on the result!  \n",
    "\n",
    "The documentation reports:\n",
    "> The p-value for a hypothesis test whose null hypothesis is that the slope is zero, using Wald Test with t-distribution of the test statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = st.linregress(x=logP, y=m36)\n",
    "\n",
    "slope, intercept, rvalue, pvalue, slope_stderr = res\n",
    "intercept_stderr = res.intercept_stderr                # for compatibility this value is extracted like this\n",
    "\n",
    "print(\"FIT RESULTS:\")\n",
    "print(\"    slope          : {:.2f} +/- {:.2f}\".format(slope, slope_stderr))\n",
    "print(\"    intercept      : {:.2f} +/- {:.2f}\".format(intercept, intercept_stderr))\n",
    "print(\"    corr. coeff. R : {:.6f}\".format(rvalue))\n",
    "print(\"    R squared      : {:.6f}\".format(rvalue**2.0))\n",
    "print(\"    p-value        : {:.6g}\".format(pvalue))\n",
    "\n",
    "x_plot = np.array([min(logP), max(logP)])\n",
    "y_plot = slope * x_plot + intercept\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(logP, m36, \"ko\", mfc=\"none\", label=\"Data\")\n",
    "plt.plot(x_plot, y_plot, \"r-\", label=\"Fit: y = {:.3g} x {:+.3g}\".format(slope, intercept))\n",
    "plt.xlabel(r\"$\\log P$\")\n",
    "plt.ylabel(r\"$m_{3.6}$\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "residuals = m36 - (slope * logP + intercept)\n",
    "# plt.figure(constrained_layout=True)\n",
    "plt.subplot(223)\n",
    "plt.plot(logP, residuals, \"ko\")\n",
    "plt.xlabel(r\"$\\log P$\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.subplot(224)\n",
    "plt.hist(residuals, bins=\"fd\", histtype=\"step\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson correlation coefficient:\n",
    "\n",
    "$$ \\large \\rho = \\dfrac{\\mathrm{cov}\\left(X, Y\\right)}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "For a sample:\n",
    "\n",
    "$$ \\large\n",
    "    r = \\dfrac{\n",
    "              \\sum\\limits_{i=1}^{n}\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\n",
    "              }\n",
    "              {\n",
    "              \\sqrt{\n",
    "              \\sum\\limits_{i=1}^{n}\\left(x_i - \\bar{x}\\right)^2\n",
    "              \\sum\\limits_{i=1}^{n}\\left(y_i - \\bar{y}\\right)^2\n",
    "              }\n",
    "              }\n",
    "$$\n",
    "\n",
    "- 0: no correlation. \n",
    "- 1: perfect correlation\n",
    "- -1: perfect anti-correlation.\n",
    "\n",
    "The square of $r$, usually refered as **R-squared** in the literature, is the *percentage of explained variance through the linear correlation*.\n",
    "\n",
    "Therefore, a $p$-value smaller than our significance level, means that **we reject the hypothesis that the slope is 0**, which can be rephrased as **we cannot reject the hypothesis that there is a linear correlation**.\n",
    "\n",
    "**Warning**: this doesn't mean that we accept that the correlation is linear. We get the above result *assuming linear correlation*!\n",
    "\n",
    "\n",
    "### If we do not need to fit, we can use the `pearsonr` function to perform the hypothesis test\n",
    "\n",
    "**Warning**: this does not alleviate the caveat that the correlation might not be linear!\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "**Task:** Complete the `pearsonr` statement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.pearsonr(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Monotonicity test\n",
    "\n",
    "Sometimes a linear correlation is not easy to see! For example, if we had no intuition about the connection between the $\\log P$ and the magnitude in RR Lyrae, we might have tried to plot period and luminosity.\n",
    "\n",
    "In that case, the model becomes $L = A P^{\\gamma}$. It's not linear anymore...\n",
    "\n",
    "Imagine if by mistake we take $P - \\log P$ instead of P...\n",
    "\n",
    "Also, if we have fewer data points, the linear correlation becomes weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10.0**logP\n",
    "L = 2.512**-m36\n",
    "\n",
    "x = P - logP\n",
    "y = L\n",
    "\n",
    "slope, intercept, _, _, _ = st.linregress(x, y)\n",
    "\n",
    "xx = np.linspace(min(x), max(x), 100)\n",
    "plt.figure()\n",
    "plt.plot(x, y, \"ko\")\n",
    "plt.plot(xx, slope * xx + intercept, \"r-\")\n",
    "plt.xlabel(r\"$P$\")\n",
    "plt.ylabel(r\"$L_{3.6}$ (arbitrary units)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test for such dependences, we can perform a **monotonicity test**. There are two widely used monotonicity checks through the *Spearman rank correlation coefficient* $r_s$ and the *Kendall rank correlation coefficient* $\\tau$ often referred as *Kendall's $\\tau$*. Their power relies on the fact that they are *non-parametric* and therefore they do not rely on an assumed model describing the data.\n",
    "\n",
    "For $r_s$ and $\\tau$:\n",
    "- 0: $x$ and $y$ are independent\n",
    "- 1: strictly increasing\n",
    "- -1: strictly decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_new, y_new, \"k.\")\n",
    "plt.show()\n",
    "# report correlation tests' p-values\n",
    "r, pvalue = st.pearsonr(x_new, y_new)\n",
    "print(\"PEARSON  : r = {:.3f}  |  p-value = {:.3g}\".format(r, pvalue))\n",
    "r, pvalue = st.spearmanr(x_new, y_new)\n",
    "print(\"SPEARMAN : r = {:.3f}  |  p-value = {:.3g}\".format(r, pvalue))\n",
    "r, pvalue = st.kendalltau(x_new, y_new)\n",
    "print(\"KENDALL  : t = {:.3f}  |  p-value = {:.3g}\".format(r, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u>**In-class discussion: How do you interprete the low Pearson's $r$-value?**</u><font>\n",
    "\n",
    "_Discuss with your teammate, then report._\n",
    "\n",
    "<details>\n",
    "<summary><b>[Spoiler]</b></summary>\n",
    "<br>\n",
    "Despite the fact that the correlation is obviouly non-linear, even the wrong model has to have a non-zero slope! Always remember your $H_0$ hypothesis! A small $p$-value or high $R^2$ does not validate the functional form of the model!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fitting a spectral line using MLE\n",
    "\n",
    "Astrophysics loves spectra since extracting features from them is extremely informative regarding the physical processes of astronomical objects.\n",
    "\n",
    "In this example we'll try to fit the LiI spectral line using a stellar spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.load(\"data/lithium_line_example.npy\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(spectrum['wavelength'], spectrum['flux'])\n",
    "ax[1].errorbar(spectrum['wavelength'], spectrum['flux'], yerr=spectrum['flux_error'], fmt='. ')\n",
    "\n",
    "ax[1].set_xlim(6705, 6709)\n",
    "ax[1].set_ylim(1200, 2700)\n",
    "\n",
    "for a in ax:\n",
    "    a.grid()\n",
    "\n",
    "ax[0].set_xlabel(r'Wavelength ($\\AA$)')\n",
    "ax[1].set_xlabel(r'Wavelength ($\\AA$)')\n",
    "ax[0].set_ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's focus on the lithium line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithium_line = spectrum[spectrum['wavelength'] > 6705]\n",
    "lithium_line = lithium_line[lithium_line['wavelength'] < 6709]\n",
    "\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt=' .')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by describing our model: a linear continuum plus a Gaussian absorption feature\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "**Tasks:**\n",
    "1. Create the model: continuum + Gaussian line\n",
    "2. Decide the return value of `ln_likelihood` for values outside the permitted ranges of the parameters\n",
    "3. Complete the `ln_likelihood` function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_flux(p, wavelength):\n",
    "    \"\"\"Return the flux for our model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    wavelength : float\n",
    "        The input wavelength(s) at which we wish to calculate our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    flux : float\n",
    "        The model flux(es) calculated by our model\n",
    "    \"\"\"\n",
    "    \n",
    "    m, b, c, sigma, loc = p\n",
    "    \n",
    "    linear_part = m*(wavelength-6707) + b\n",
    "    gaussian_part = ...\n",
    "    \n",
    "    return linear_part + gaussian_part\n",
    "\n",
    "def ln_likelihood(p, data):\n",
    "    \"\"\"Return the likelihood of our model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    data : ndarray\n",
    "        The observed data from which we wish to calculate the likelihood of our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ln_likelihood : float\n",
    "        The log likelihood of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    m, b, c, sigma, loc = p\n",
    "    if sigma <= 0: return ...\n",
    "    if loc < 6706 or loc > 6708: return ...\n",
    "    \n",
    "    # First, calculate the model fluxes at the observed wavelengths\n",
    "    model_fluxes = model_flux(p, data['wavelength'])\n",
    "    \n",
    "    # Now capture our observed fluxes\n",
    "    observed_fluxes = data['flux']\n",
    "    observed_flux_errors = data['flux_error']\n",
    "    \n",
    "    # Compare the two\n",
    "    ln_likelihood = ...\n",
    "        \n",
    "    return sum(ln_likelihood)\n",
    "\n",
    "def neg_ln_likelihood(p, data):\n",
    "    \"\"\"Wrapper for ln_likelihood to return the negative of the log likelihood.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p : tuple\n",
    "        The model parameters\n",
    "    data : ndarray\n",
    "        The observed data from which we wish to calculate the likelihood of our model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_ln_likelihood : float\n",
    "        The negative of the log likelihood of our model\n",
    "    \"\"\"\n",
    "\n",
    "    return -ln_likelihood(p, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a trial solution and use scipy minimize to find a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 40\n",
    "b = 2400\n",
    "c = 400\n",
    "sigma = 0.2\n",
    "loc = 6707.1\n",
    "\n",
    "p0 = (m, b, c, sigma, loc)\n",
    "\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(p0, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C1')\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(neg_ln_likelihood, p0, method='Nelder-Mead', args=lithium_line)\n",
    "\n",
    "best_p = res.x\n",
    "model_wavelengths = np.linspace(np.min(lithium_line['wavelength']), np.max(lithium_line['wavelength']), 1000)\n",
    "model_fluxes = model_flux(best_p, model_wavelengths)\n",
    "\n",
    "plt.plot(model_wavelengths, model_fluxes, color='C1')\n",
    "\n",
    "plt.errorbar(lithium_line['wavelength'], lithium_line['flux'], \n",
    "             yerr=lithium_line['flux_error'], \n",
    "             fmt='o', marker='.')\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\AA$)')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the likelihood vary with model parameters?\n",
    "\n",
    "Let's see how the likelihood will vary with the the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12, 4), constrained_layout=True)\n",
    "labels = ['m', 'b', 'c', r'$\\sigma$', r'$\\mu$']\n",
    "\n",
    "for j, (label, ax) in enumerate(zip(labels, axes)):\n",
    "    # get the best-fitting values\n",
    "    trial_p = best_p.copy()\n",
    "\n",
    "    # take a range around the j-th parameter's best-fiting value\n",
    "    test_set = best_p[j] * np.linspace(0.99, 1.01, 1000)\n",
    "\n",
    "    # get the log-likelihood for the different values (keeping fixed the rest of the parameters)\n",
    "    ll = np.zeros_like(test_set)\n",
    "    for i, test_val in enumerate(test_set):\n",
    "        trial_p[j] = test_val\n",
    "        ll[i] = ln_likelihood(trial_p, lithium_line)\n",
    "\n",
    "\n",
    "    ax.plot(test_set, ll)\n",
    "    ax.axvline(best_p[j], color='k', linestyle='dashed')    \n",
    "    ax.set_title(labels[j])\n",
    "    ax.set_xlabel(r'$\\lambda (\\AA)$')\n",
    "    ax.grid()\n",
    "\n",
    "axes[0].set_ylabel('ln $\\mathcal{L}$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-astrostat24] *",
   "language": "python",
   "name": "conda-env-miniconda3-astrostat24-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
