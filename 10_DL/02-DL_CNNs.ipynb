{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>**DL CNNs**</font> </h6>\n",
    "\n",
    "In this session we dive into one of the most commonly used applications of DL which corresponds to the **Convolutional Neural Networks**. We will use mock images to classify galaxies, QSOs, and stars. <br>\n",
    "The goals are:\n",
    "\n",
    "- to get a grasp of what **CNNs are** \n",
    "- to **build and train** a DL network\n",
    "\n",
    "In the example that will follow we are mainly going through these steps:\n",
    "\n",
    "    1. Load (mock) Data\n",
    "    2. Define Model\n",
    "    3. Compile Model\n",
    "    4. Fit Model \n",
    "    5. Iterate steps 2-3-4 (by adjusting various parameters or the model architecture)\n",
    "    6. Evaluate Model\n",
    "    8. Predict class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small introduction\n",
    "\n",
    "## What are Convolutional Neural Networks (CNN)\n",
    "\n",
    "> Is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. <br>\n",
    ">\n",
    "> _[A Comprehensive Guide to Convolutional Neural Networks â€” the ELI5 way, by Sumit Saha](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of CNN architecture\n",
    "\n",
    "![CNN_schematic](https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n",
    "\n",
    "The design of CNN allows to apply similar concepts to Neural Networks with special data processing techniques on data and between layers to learn from image data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "The convultion in CNNs is a technique inspired by the organization of the visual cortex, as neurons respond to stimulius in a given field of view. The convolution is a way to propogate information from nearby pixels in an image. \n",
    "\n",
    ">The aim of CNN is to **reduce the dimensions** and **keep the most important features** that help in good predictions.  \n",
    "\n",
    "Essentially a convolution is a matrix multiplication between the image and a *kernel* (another matrix, smaller than the image). Note, the shape of your input data has changed after going through the convolution (_'valid padding'_, in contrast to _'same padding'_ where the original dimensions are kept)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/kernel_snapshot.png\"> </img>\n",
    "    </div>\n",
    "\n",
    "![convolution](https://miro.medium.com/max/1052/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "Convoluting a 5x5x1 image with a 3x3x1 kernel to get a 3x3x1 convolved feature. The kernel (shown in yellow) takes into account only the pixels in the two diagonals (marked as 'x1' in the lower right corner of the yellow matrix). Therefore, in the first (frozen) image there are 9 pixels with the kernel considering 5 of them with values : 1+1+0+1+1 = 4 (the value transfered to the convoled feature).\n",
    "</div>\n",
    "\n",
    "The kernel is not necessary to move one pixel at a time. By changing the _stride_ we can select any kind of movement, which includes both the width and the height. A (1,1) stride will move one pixel right (stating always from the top left corner) and after completing the row it will move one pixel down (and left again). A (2,2) will do the similar thing but with two pixels moves. However, in this case we also **downsampling** the extracted feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "The function used to impose a non-linear transformation to the input data. Perhaps the most typical one used is the ReLU (Rectified Linear Unit), which has the advantage of not activating all neurons at the same time.  \n",
    "\n",
    "### Pooling\n",
    "\n",
    "Sometimes data is big and we want to speed up the process. Can we *pool* some cells together to reduce our data size between convolutions? Yes! The technique is called (obviously...) _pooling_ and it can be performed by either taking the average of all the pixels that the pooling layer is over the feature layer (**average pooling**) or the maximum value found in any of the pixels (**max pooling**). \n",
    "\n",
    "![pooling_2](https://miro.medium.com/max/1192/1*KQIEqhxzICU7thjaQBfPBQ.png)\n",
    "<div style=\"text-align: center;\">\n",
    "Examples of max and average pooling. \n",
    "</div>\n",
    "\n",
    "![pooling_1](https://miro.medium.com/max/792/1*uoWYsCV5vBU8SHFPAPao-w.gif)\n",
    "<div style=\"text-align: center;\">\n",
    "A 3x3 max pooling acting over a 5x5 feature map. \n",
    "</div>\n",
    "\n",
    "The benefits of pooling layers are: i. the **decrease of dimensions** that help the decrease the computational power, ii. they extract the most **dominant features which are rotational and positional invariant**. \n",
    "\n",
    "There are two flavors of pooling layers, either local (with dimensions smaller that the feature dimensions) or _global_ that act on the whole feature layer (and they actually convert it to a single value), which is more aggressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layer\n",
    "\n",
    "This is the fundamental layer where each neuron in the layer is connected to every neuron in the previous layer. This type of layer is also known as a dense layer because each neuron is connected to all neurons in the preceding layer.\n",
    " \n",
    "\n",
    "### Dropout\n",
    "\n",
    "One way to prevent overfitting is the dropout method - remove individual nodes from the network (with some probability) at each training stage. This could be at the level of the input node or at hidden layers.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Each layer's weights (and therefore outputs) are updated every training iteration. More layers can mean larger changes down the network (nonlinear behavior), for small changes in weights, so small learning rates may be needed which makes training hard. Instead we may enforce each layer to produce **predictable** output from layer to layer using batch normalization giving more stable behavior and reducing training time. Predictable in this case means that the distribution of outputs from the previous layer has specific properties: unit variance, zero mean. In other words it is a technique to standardize the input to a layer. ([Ioffe & Szegedy 2015](https://arxiv.org/abs/1502.03167))\n",
    "\n",
    "(Source: images and material mostly from [this web article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example classification networks\n",
    "\n",
    "AlexNet & LeNet: image classification networks - \"In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010, AlexNet was trained to classify 1.2 million high-resolution images into 1000 different classes. It achieved top-1 and top-5 error rates of 37.5% and 17%, which outperforms state-of-the-art methods at that time.\" [article](https://medium.com/mlearning-ai/alexnet-and-image-classification-8cd8511548b4)\n",
    "    \n",
    "![example network architectures](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)\n",
    "\n",
    "## Visualization of layers\n",
    "\n",
    "There follow a few links that help to visualize how CNN works: \n",
    "\n",
    "- [CNN explainer](https://poloclub.github.io/cnn-explainer/)\n",
    "\n",
    "- [CNN demo on MNIST dataset](https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy morphology estimation\n",
    "\n",
    "**TASK 1: Build a network to classify stars, spiral and elliptical galaxies from synthetic data with noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import keras\n",
    "from IPython.display import clear_output\n",
    "import keras.utils as ult\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization,Conv3D, MaxPooling3D, Dense, Add, Activation\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an auxiliary function to plot the accuracy and loss value during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.losses2 = []\n",
    "        self.val_losses2 = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.losses2.append(logs.get('categorical_accuracy'))\n",
    "        self.val_losses2.append(logs.get('val_categorical_accuracy'))\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.x, self.losses2, label=\"Training accuracy\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses2, label=\"Validation accuracy\",linestyle='--')\n",
    "        plt.ylim(0,1)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.x, self.losses, label=\"Training loss\",linestyle='-')\n",
    "        plt.plot(self.x, self.val_losses, label=\"Validation loss\",linestyle='--')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,galaxy_labels):\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[0]))\n",
    "    plt.imshow(images[0,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[0,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[0,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[1]))\n",
    "    plt.imshow(images[1,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[1,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[1,:,:,2], vmax=255)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(label_trans(galaxy_labels[5]))\n",
    "    plt.imshow(images[5,:,:,0], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(images[5,:,:,1], vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(images[5,:,:,2], vmax=255)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_trans(label_id):\n",
    "    if label_id==0: return \"star\"\n",
    "    if label_id==1: return \"spiral galaxy\"\n",
    "    if label_id==2: return \"elliptical galaxy\"\n",
    "    else: return \"unknown\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "**data**: are images at different wavelenghts i.e. 3D with 2 spatial (41x41 pixels) and 1 spectral (3 bands) dimension     \n",
    "**labels**: take values 0: star, 1: spiral galaxy, 2: elliptical galaxy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/galaxy_cubes.npz') as data:\n",
    "    images = data['images']\n",
    "    galaxy_labels = data['labels']\n",
    "\n",
    "print([images.shape, galaxy_labels.shape])\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add white noise to observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= images+np.random.randn(10000,41,41,3)*20\n",
    "images= np.clip(images, 0, 255)\n",
    "\n",
    "show_images(images,galaxy_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Create training and testing (validation) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT:_ to test various architectures fast keep the train/test sizes rather **small**, i.e. use a huge fraction for test to get a very small number for train+validation (a few hundrends). At the end you would like to retrain with the **full dataset** (that will take some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using two times the train_test_split to get initially \n",
    "# the test sample and then train and validation.\n",
    "# \n",
    "# you can use random_state if you want to reproduce the same exact splits\n",
    "# and shuffle if you want to mix the data before splits\n",
    "\n",
    "X_train_full, X_test_img, y_train_full_lbl, y_test_lbl = train_test_split(\n",
    "        ... , ..., test_size=0.95) #, shuffle = True, random_state=42)\n",
    "\n",
    "# split into train and validation\n",
    "X_train_img, X_valid_img, y_train_lbl, y_valid_lbl = train_test_split(\n",
    "        ... , ... , test_size=0.3) #, shuffle = True, random_state=24 )\n",
    "\n",
    "print(f'From {len(images)} images, we use as:')\n",
    "print(f'test: \\t\\t {len(X_test_img)}')\n",
    "print(f'train: \\t\\t  {len(X_train_img)}')\n",
    "print(f'validation:\\t  {len(X_valid_img)}')\n",
    "\n",
    "# NOTE: this is a data manipulation as keras needs the number of objects \n",
    "# with properties at each \"channel\", and their correspoding number. \n",
    "# As keras thinks of images as RGB it uses 3 as last number. \n",
    "# To avoid keras to assume anything add specifically ',1' at the end.\n",
    "\n",
    "X_train = X_train_img.reshape(len(X_train_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "X_valid = X_valid_img.reshape(len(X_valid_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "X_test  = X_test_img.reshape(len(X_test_img), images.shape[1],images.shape[2],images.shape[3],1)\n",
    "\n",
    "\n",
    "# NOTE: converting labels to categorical representation, \n",
    "# a vector whose position indicates its class\n",
    "# 0: star, ---------------> [1, 0, 0] \n",
    "# 1: spiral galaxy, ------> [0, 1, 0]\n",
    "# 2: elliptical galaxy ---> [0, 0, 1]   \n",
    "y_train = ult.to_categorical(y_train_lbl,num_classes=3)\n",
    "y_valid = ult.to_categorical(y_valid_lbl,num_classes=3)\n",
    "y_test  = ult.to_categorical(y_test_lbl,num_classes=3)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network layers and characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((images.shape[1], images.shape[2], images.shape[3], 1),name='main_input')\n",
    "\n",
    "conv00  = Conv3D(16, (3, 3, 2), strides=(1, 1, 1), padding='same', name='conv00')(inputs)\n",
    "act00 = Activation('relu')(conv00)\n",
    "pool00  = MaxPooling3D(pool_size=(3, 3, 1), strides=(2, 2, 1), padding='same')(act00)\n",
    "\n",
    "...\n",
    "# you can also use :\n",
    "# Do0 = Dropout(rate=0.5)( acting_on_layer )\n",
    "# bn00 = BatchNormalization()( acting_on_layer )\n",
    "... \n",
    "\n",
    "\n",
    "fl0 = Flatten(name='fl0')( acting_on_layer )\n",
    "\n",
    "fc0 = Dense(32,activation='linear')(fl0)\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Dn0 = Dense( ... ,activation='softmax', name='Dn0' )( acting_on_layer )\n",
    "\n",
    "my_model = Model(inputs=[inputs], outputs=[Dn0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select optimizer and compile the model\n",
    "\n",
    "_HINT:_ check the documentation ([keras:accuracy_metrics](https://keras.io/api/metrics/accuracy_metrics/)) and remember that we are using the categorical labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=Adam(lr=1e-4), \n",
    "                 metrics =['categorical_accuracy'])\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "_HINT:_ to test training fast keep the **batch_size larger**, and the **epochs smaller**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "                                                    \n",
    "history=my_model.fit( ... , ... , \n",
    "                    batch_size= 64, \n",
    "                    epochs= 50,\n",
    "                    validation_data=[ ... , ... ],\n",
    "                    callbacks=[plot_losses],shuffle=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_HINT_: if you want to speed up the process a bit select the number of TO objects of the test set (eg first 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO = len(X_test) # or smaller...\n",
    "\n",
    "ls,acc=my_model.evaluate( X_test[0:TO], y_test[0:TO])  \n",
    "print(\"Loss value: %.2f\" % (ls))  \n",
    "print(\"Accuracy: %.1f\" % (acc*100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict label for particular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select object\n",
    "obj = ...        # must be less than len( X_test)\n",
    "\n",
    "preds = my_model.predict( X_test[obj:obj+1,:,:,:,:])\n",
    "print(f\"Probability per class: {', '.join([str(i*100)[0:5]+'%' for i in preds[0]])}\")\n",
    "print(f'Highest for class: {label_trans( np.argmax(preds))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the activations for particular inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `model.layers` we print all layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select for which one to print the activations.\n",
    "\n",
    "_HINT: select convolutional layers to check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_layer = ...  # eg 1\n",
    "\n",
    "my_model.layers[sel_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a random sample to present.\n",
    "\n",
    "_HINT: to avoid issues with plots check that the number of nodes in the concolution layer is properly trasnfered to the plotting for-loop of activation layers_\n",
    "\n",
    "_Compare this with what we see with the [CNN explainer](https://poloclub.github.io/cnn-explainer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = np.random.randint(0,len(X_test)-1)\n",
    "\n",
    "plt.imshow(X_test[s,:,:,0,0])\n",
    "plt.title(f'Input image index: {s}')\n",
    "plt.show()\n",
    "\n",
    "# using the specific layer as an output\n",
    "lr = my_model.layers[sel_layer].output  \n",
    "activation_model_lr = Model(inputs=[inputs], outputs=lr)\n",
    "\n",
    "# extracting the activations of specific layer (as a model)\n",
    "activations_lr = activation_model_lr.predict( X_test[s:s+1,:,:,:,:]) \n",
    "\n",
    "# NOTE: check the number of nodes in the CNN\n",
    "for i in np.arange(16):\n",
    "    img=activations_lr[0,:,:,0,i]\n",
    "    plt.imshow(img)\n",
    "    plt.title('Number ' + str(i))\n",
    "    plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
