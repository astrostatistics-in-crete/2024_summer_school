{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SBI: In-class exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import basinhopping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from scipy.stats import norm\n",
    "from getdist import plots, MCSamples\n",
    "from scipy.integrate import quad\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the main notebook, our mock observations were generated by simply <span style=\"color:darkorange\">adding some Gaussian noise to the theoretical data</span>. However, in a <span style=\"color:darkorange\">real experiment</span>, our observations may be affected various <span style=\"color:darkorange\">other effects</span>, which may be caused by the instruments, the environment, physical processes, etc. These effects are often very **hard to model with a simple analytical function**, and may introduce additional **biases** in the data.\n",
    "\n",
    "In this exercise, we will assume that our observations are contaminated by two new effects: random instrumental drift and intermittent calibration errors.\n",
    "\n",
    "1. **Random instrumental drift**: The instrument that we use to measure the data is not perfect, and its <span style=\"color:darkorange\">calibration may drift over time</span> due to environmental factors or aging hardware. This drift will accumulate over time and so the measurements that we take at different times may be affected by a random offset. We can model this effect by adding a cumulative sum of normally distributed random values to the true magnitudes.\n",
    "    <details> \n",
    "    <summary>Why? [Click to expand]\n",
    "     <br><br>\n",
    "    </summary>\n",
    "    This simulates the effect of a drift that changes the measurements in a continuous but random manner. The random values are drawn from a normal distribution with a mean of zero, ensuring that the drift can go in either direction (positive or negative) and has a variance that reflects the magnitude of the drift per observation.\n",
    "    <br> <br>\n",
    "    </details>\n",
    "\n",
    "\n",
    "2. **Intermittent calibration errors**: The instrument may also suffer from occasional <span style=\"color:darkorange\">large calibration errors</span> that occur at random intervals. This may be caused by a variety of factors, such as power surges, cosmic rays, mechanical disturbances, miscalibrations during the data collection process etc. We model these errors by introducing occasional large spikes in the observed magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed\n",
    "np.random.seed(1235)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Constants\n",
    "c = 299792.458  # Speed of light in km/s\n",
    "\n",
    "# True cosmological parameters\n",
    "true_Omega_m = 0.3\n",
    "true_H0 = 70.0  # Hubble constant in km/s/Mpc\n",
    "\n",
    "# Redshifts of the supernovae\n",
    "z = np.linspace(0.01, 1.0, 50)\n",
    "\n",
    "# Noise level in the observed magnitudes\n",
    "sigma = 0.1\n",
    "\n",
    "# Luminosity distance function\n",
    "def luminosity_distance(z, Omega_m, H0):\n",
    "    \"\"\" Calculate the luminosity distance for a given redshift, Omega_m and H0 \"\"\"\n",
    "    integrand = lambda z_prime: 1.0 / np.sqrt(Omega_m * (1 + z_prime)**3 + (1 - Omega_m))\n",
    "    d_L = np.array([quad(integrand, 0, z_i)[0] for z_i in z])\n",
    "    return (c * (1 + z) * d_L) / H0\n",
    "\n",
    "# Generate synthetic data\n",
    "# Generate synthetic data with non-linear transformation\n",
    "d_L_true = luminosity_distance(z, true_Omega_m, true_H0)\n",
    "m_true = 5 * np.log10(d_L_true / 10)\n",
    "\n",
    "# Introduce random walk drift\n",
    "def random_walk_drift(m, z):\n",
    "    drift = np.cumsum(np.random.normal(0, 0.2, len(m)))\n",
    "    return m + drift\n",
    "\n",
    "# Introduce intermittent calibration errors\n",
    "def intermittent_calibration_errors(m, z):\n",
    "    errors = np.zeros_like(m)\n",
    "    error_indices = np.random.choice(len(m), size=int(len(m) * 0.1), replace=False)\n",
    "    errors[error_indices] = np.random.normal(0, 1, len(error_indices))\n",
    "    return m + errors\n",
    "\n",
    "# Apply both effects to the observed magnitudes\n",
    "m_obs_drift = random_walk_drift(m_true, z)\n",
    "m_obs = intermittent_calibration_errors(m_obs_drift, z) + np.random.normal(0, sigma, len(m_true))\n",
    "\n",
    "\n",
    "# Plot synthetic data\n",
    "plt.errorbar(z, m_obs, yerr=0.5, fmt='.', label='Observed magnitudes')\n",
    "plt.plot(z, m_true, label='True magnitudes')\n",
    "plt.xlabel('Redshift $z$')\n",
    "plt.ylabel('Apparent magnitude $m$')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "# Part 1: Solving with Gaussian likelihood\n",
    "\n",
    "**Problem:** Given the observed magnitudes $m_{\\text{obs}}$ and redshifts $z$, infer the cosmological parameters $\\Omega_m$ and $H_0$ using the traditional Bayesian approach assuming a Gaussian likelihood:\n",
    "$$\\mathcal{L}(\\Omega_m, H_0) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(m_{\\text{obs},i} - m_{\\text{th},i})^2}{2\\sigma_i^2}\\right), \\tag{3}$$\n",
    "\n",
    "**Tasks:**\n",
    "1. Define the prior, a Gaussian likelihood with some $\\sigma$ and the posterior.\n",
    "2. Run MCMC to sample from the posterior.\n",
    "3. Plot the corner plot of the posterior samples and see if the results are consistent with the true values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "\n",
    "<details>\n",
    "    <summary><b>Click to see the tip</b></summary>\n",
    "    \n",
    "Do ***exactly*** what was done in the main notebook:\n",
    "\n",
    "- For the prior, use a flat prior over $\\Omega_m$ and $H_0$.\n",
    "- For the Gaussian likelihood, you can assume a $\\sigma$ looking at the data (I would assume $\\sigma=0.5$, as in the main notebook).\n",
    "- For MCMC, you can use the `emcee` package.\n",
    "    \n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-prior function\n",
    "def log_prior(params):\n",
    "    ...\n",
    "    \n",
    "# Define the log-likelihood function\n",
    "def log_like(params, z, m_obs):\n",
    "    ...\n",
    "\n",
    "# Log-posterior function\n",
    "def log_post(params, z, m_obs):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with `emcee` \n",
    "ndim, nwalkers = ... , ...\n",
    "ncpus = multiprocessing.cpu_count()\n",
    "\n",
    "# Initialize walkers \n",
    "pos = ...\n",
    "\n",
    "with Pool(ncpus) as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=(z, m_obs), pool=pool)\n",
    "    sampler_output = sampler.run_mcmc(pos, nsteps=..., progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples and convert to GetDist MCSamples\n",
    "flat_samples = sampler.get_chain(discard=..., flat=True)\n",
    "\n",
    "# GetDist plotting\n",
    "samples = MCSamples(samples=flat_samples, names=['\\Omega_m', 'H_0'], labels=['\\Omega_m', 'H_0'])\n",
    "g = plots.get_subplot_plotter(width_inch=4)\n",
    "g.triangle_plot(samples, filled=True, markers={'\\Omega_m': true_Omega_m, 'H_0': true_H0})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
    "\n",
    "# Part 2: Solving with Neural Posterior Estimation\n",
    "\n",
    "**Problem:** Given the observed magnitudes $m_{\\text{obs}}$ and redshifts $z$, infer the cosmological parameters $\\Omega_m$ and $H_0$ using the Neural Posterior Estimation approach.\n",
    "\n",
    "**Tasks:**\n",
    "1. Construct a **simulator** that generates the observed magnitudes $m_{\\text{obs}}$ given the cosmological parameters $\\Omega_m$ and $H_0$. Encode the correct gaussian noise level, the random instrumental drifts and the intermittent calibration errors in the simulator.\n",
    "2. Generate a **training dataset** of observed magnitudes $m_{\\text{obs}}$ and corresponding cosmological parameters $\\Omega_m$ and $H_0$, by running the simulator for a range of randomly drawn parameters $\\Omega_m$ and $H_0$. Then, normalize the training data.\n",
    "3. Define an MPL neural network that will be used as a feature extractor.\n",
    "4. Define the **Neural Posterior Estimator** class that will use the feature extractor to estimate the posterior with a normalizing flow.\n",
    "5. Make the training dataset suitable for the NPE, and train the NPE.\n",
    "6. Sample from the NPE, conditioning on the observed magnitudes $m_{\\text{obs}}$ that we had in the beginning.\n",
    "7. Plot the corner plot of the posterior samples and see if the results are consistent with the true values.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "\n",
    "<details>\n",
    "    <summary><b>Click to see the tip</b></summary>\n",
    "\n",
    "Again, do ***exactly*** what was done in the main notebook. The actual changes that you need to implement are:\n",
    "* In the simulator, introduce the random instrumental drifts and intermittent calibration errors.\n",
    "* For the test data, use instead the observed magnitudes $m_{\\text{obs}}$ that we showed in the beginning.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward model to simulate observed magnitudes\n",
    "def luminosity_distance_simulator(params, z):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # use random seed for reproducibility\n",
    "\n",
    "# Generate training data\n",
    "n_train = 50_000\n",
    "param_samples = np.random.uniform(low=[0, 50], high=[1, 100], size=(n_train, 2)) # Parameter proposal\n",
    "x_samples = np.array([luminosity_distance_simulator(theta, z) for theta in tqdm(param_samples)])\n",
    "\n",
    "# Convert to tensors\n",
    "param_samples = torch.tensor(param_samples, dtype=torch.float32)\n",
    "x_samples = torch.tensor(x_samples, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Νormalize data to zero mean and unit variance\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, hidden_dim, output_dim, num_layers, activation=nn.GELU()):\n",
    "    \"\"\"Builds a multi-layer perceptron (MLP) neural network.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralPosteriorEstimator(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Neural Posterior Estimator using a normalizing flow as the posterior density estimator.\n",
    "\n",
    "    Parameters:\n",
    "        featurizer (nn.Module): Neural network for feature extraction.\n",
    "        d_context (int): Dimension of the context vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, featurizer, d_context=16):\n",
    "        super().__init__()\n",
    "        self.featurizer = featurizer # Featurizer network for context extraction\n",
    "        self.flow = self._build_flow(...) # Normalizing flow\n",
    "    \n",
    "    def _build_flow(self, d_in, d_hidden, d_context, n_layers):\n",
    "        \"\"\" Instantiate a normalizing flow with Masked Autoregressive Transformations. \"\"\"\n",
    "        base_dist = StandardNormal(shape=...) # base distribution of NF = standard normal \n",
    "        transforms = [] # List to hold the series of transformations\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            # Reverse permutation to shuffle the features\n",
    "            transforms.append(ReversePermutation(features=...))\n",
    "            \n",
    "            # Masked Autoregressive Transform for flexible and expressive transformations\n",
    "            transforms.append(MaskedAffineAutoregressiveTransform(features=..., hidden_features=..., context_features=...))\n",
    "            \n",
    "        # Combine all transformations into a composite transform\n",
    "        transform = CompositeTransform(transforms)\n",
    "        \n",
    "        return Flow(transform, base_dist)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.featurizer(x) # Extract features using the featurizer\n",
    "    \n",
    "    def loss(self, x, params):\n",
    "        context = self(x)  # Extract context from x using the featurizer\n",
    "        return ... # Compute negative log probability\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, params = batch \n",
    "        loss = ...  # Compute mean loss\n",
    "        self.log(\"train_loss\", loss) # Log the training loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, params = batch\n",
    "        loss = ... # Compute mean validation loss\n",
    "        self.log(\"val_loss\", loss) # Log the validation loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = build_mlp(input_dim=..., hidden_dim=..., output_dim=..., num_layers=...)\n",
    "npe = NeuralPosteriorEstimator(featurizer=featurizer, d_context=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fraction = 0.1 # Fraction of data to use for validation\n",
    "batch_size = 128 \n",
    "n_samples_val = int(val_fraction * len(...)) \n",
    "\n",
    "dataset = TensorDataset(..., ...) \n",
    "dataset_train, dataset_val = random_split(dataset, [len(x_samples) - n_samples_val, n_samples_val]) \n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=..., accelerator='cpu')\n",
    "trainer.fit(model=..., train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = np.array([true_Omega_m, true_H0])\n",
    "x_test = m_obs\n",
    "\n",
    "x_test_norm = ... # normalize the test data\n",
    "context = npe.featurizer(x_test_norm).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the trained normalizing flow\n",
    "test_samples = ...\n",
    "test_samples = test_samples.detach().numpy().squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MCSamples instance\n",
    "samples_npe = MCSamples(samples=test_samples, labels=['\\Omega_m', 'H_0'], names=['\\Omega_m', 'H_0'])\n",
    "\n",
    "true_Omega_m = 0.3\n",
    "true_H0 = 70\n",
    "\n",
    "# Create a GetDist plot\n",
    "g = plots.get_subplot_plotter(width_inch=6)\n",
    "g.triangle_plot([samples_npe, samples], filled=True, markers={'\\Omega_m': true_Omega_m, 'H_0': true_H0}, legend_labels=['NPE', 'MCMC'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrostat24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
